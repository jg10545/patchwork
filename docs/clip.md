# CLIP Implementation`patchwork` contains a prototype TensorFlow implementation of OpenAI's [Contrastive Language Image Pretraining](https://arxiv.org/abs/2103.00020)## Implementation notes* I'm using [sentencepiece](https://github.com/google/sentencepiece) to handle the byte-pair encoding.* I've not yet added the attention head for the vision model* So far I've only tested with convolutional neural networks for the vision model (no [vision transformers](https://keras.io/examples/vision/image_classification_with_vision_transformer/) yet)* My language transformer is based off of [this example](https://keras.io/examples/nlp/text_classification_with_transformer/). The attention blocks appear to have a slightly different structure than the residual attention blocks in OpenAI's code.* For now I'm saving and loading all the model components with the TF SavedModel format instead of HDF5 (which I've been using for the other models) to better accomodate the custom layers.## Example code### Training the byte-pair encoderBefore training CLIP, built a byte-pair encoder using sentencepiece:```{python}import sentencepiece as spmbpetext = "/all/my/labels/saved/as/a/text/file.txt"outname = "bpe"vocab_size = 100spm.SentencePieceTrainer.train(input=bpetext,                                model_prefix=outname,                                vocab_size=vocab_size)                                # model will be saved to 'bpe.model'# vocab will be saved to 'bpe.vocab'```### Training CLIP```{python}import tensorflow as tfimport patchwork as pwimport json# load paths to train and test data. for each, there should be a list# or array of filenames and a corresponding list/array of text# descriptions.trainfiles = [x.strip() for x in open("mytrainfiles.txt").readlines()]trainlabels = [x.strip() for x in open("mytrainlabels.txt").readlines()]testfiles = [x.strip() for x in open("mytestfiles.txt").readlines()]testlabels = [x.strip() for x in open("mytestlabels.txt").readlines()]# pick a place to save models and tensorboard logslog_dir = "/path/to/my/log_dir/"# choose an augmentation strategyaug = {"rot90":True, "zoom_scale":0.7, "flip_left_right":True, "flip_up_down":True}# initialize trainer and traintrainer = pw.feature.CLIPTrainer(    log_dir,    "/path/to/bpe.model",    trainfiles,    trainlabels,    testdata=testfiles,    testlabels=testlabels,    augment=aug,    weight_decay=1e-6,    lr=1e-4,    opt_type="adam",    imshape=(120,120),    batch_size=192,    num_parallel_calls=6)# train for 100 epochstrainer.fit(100)```### Run a test queryI expect this function will evolve- but as a first pass, you can pass a text queryto your model and plot the 9 test images with the highest cosine similarity toyour query:```{python}trainer.query_test_set("somebody once told me, the world is gonna roll me")```The first time you run this after training may take a while as it has to build the index.